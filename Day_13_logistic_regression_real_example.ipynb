{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNV2akrCZahU7XGS6FDB4FQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QusaiALBahri/Day_13_logistic_regression/blob/main/Day_13_logistic_regression_real_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a8fb8ef"
      },
      "source": [
        "# Task\n",
        "We will write a simple Logistic Regression tutorial to classify emails into spam or not spam using the dataset at \"/content/emails.csv\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96dde7c8"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "### Subtask:\n",
        "Load the `emails.csv` dataset into a pandas DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66a6dcdb"
      },
      "source": [
        "**Reasoning**:\n",
        "Import pandas and load the CSV file into a DataFrame, then display the head to inspect the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d408a58f"
      },
      "source": [
        "import pandas as pd\n",
        "#please use (upload to colab) the csv file in github repo.\n",
        "df = pd.read_csv('/content/emails.csv')\n",
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88b7a843"
      },
      "source": [
        "## Explore and preprocess data\n",
        "\n",
        "### Subtask:\n",
        "Inspect the data, handle missing values, and prepare the text data for the model (e.g., using TF-IDF).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eafb057b"
      },
      "source": [
        "**Reasoning**:\n",
        "Inspect the data for shape and missing values, and then convert the text data to TF-IDF representation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b1f561c"
      },
      "source": [
        "print(\"Shape of the DataFrame:\", df.shape)\n",
        "\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Handle missing values - drop rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "print(\"\\nShape of the DataFrame after dropping missing values:\", df.shape)\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Convert text data to TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000) # Limiting features to 5000\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'])\n",
        "\n",
        "print(\"\\nShape of the TF-IDF matrix:\", tfidf_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a59193ab"
      },
      "source": [
        "## Split the data\n",
        "\n",
        "### Subtask:\n",
        "Split the data into training and testing sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fcea2cd"
      },
      "source": [
        "**Reasoning**:\n",
        "Split the data into training and testing sets using `train_test_split` based on the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3846079c"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, df['spam'], test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bc61dfe"
      },
      "source": [
        "## Build and train the model\n",
        "\n",
        "### Subtask:\n",
        "Create a logistic regression model and train it on the training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65566156"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires creating and training a Logistic Regression model. This involves importing the necessary class, instantiating the model, and fitting it to the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42de3602"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Instantiate a LogisticRegression object\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb14c8d9"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the model's performance on the testing data using appropriate metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08b0e5b8"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary metrics from sklearn.metrics, make predictions on the test set, and calculate the evaluation metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a35df9ec"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Make predictions on the test features\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54dd3ccb"
      },
      "source": [
        "## Make predictions\n",
        "\n",
        "### Subtask:\n",
        "Show how to make predictions on new data using the trained model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b847aaa"
      },
      "source": [
        "**Reasoning**:\n",
        "Create new email texts, transform them using the fitted TF-IDF vectorizer, and predict their spam classification using the trained model. Then, display the original texts and their predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a67b535"
      },
      "source": [
        "# 1. Create new email texts\n",
        "new_emails = [\n",
        "    \"Subject: special offer just for you! limited time!\", # Likely spam\n",
        "]\n",
        "\n",
        "# 2. Transform new email texts using the fitted tfidf_vectorizer\n",
        "new_emails_tfidf = tfidf_vectorizer.transform(new_emails)\n",
        "\n",
        "# 3. Use the trained model to make predictions\n",
        "predictions = model.predict(new_emails_tfidf)\n",
        "\n",
        "# 4. Print the original texts and their predictions\n",
        "print(\"New Email Predictions:\")\n",
        "for email, prediction in zip(new_emails, predictions):\n",
        "    print(f\"Email: {email}\")\n",
        "    print(f\"Prediction: {'Spam' if prediction == 1 else 'Not Spam'}\")\n",
        "    print(\"-\" * 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e3af83f"
      },
      "source": [
        "## Summarize findings\n",
        "\n",
        "### Subtask:\n",
        "Provide a summary of the tutorial and the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d643c49"
      },
      "source": [
        "**Reasoning**:\n",
        "Write a markdown summary of the tutorial, including the steps, evaluation metrics, and prediction process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0071c728"
      },
      "source": [
        "summary = \"\"\"\n",
        "## Logistic Regression Tutorial: Email Spam Classification\n",
        "\n",
        "This tutorial demonstrates how to build a Logistic Regression model to classify emails as spam or not spam using the provided dataset.\n",
        "\n",
        "**1. Data Loading:**\n",
        "The tutorial began by loading the email dataset from the `/content/emails.csv` file into a pandas DataFrame. The dataset contains two columns: 'text' (the email content) and 'spam' (the target variable, where 1 indicates spam and 0 indicates not spam).\n",
        "\n",
        "**2. Data Exploration and Preprocessing:**\n",
        "The data was inspected for its shape and missing values. Fortunately, no missing values were found. The 'text' column, containing the raw email content, was then transformed into a numerical representation using the TF-IDF (Term Frequency-Inverse Document Frequency) vectorization technique. This process converts text into a matrix of numerical features, which is suitable for machine learning models. A maximum of 5000 features was used for the TF-IDF representation.\n",
        "\n",
        "**3. Data Splitting:**\n",
        "The dataset was split into training and testing sets. 80% of the data was allocated for training the model, and the remaining 20% was held out for evaluating its performance. This split ensures that the model is evaluated on data it has not seen during training.\n",
        "\n",
        "**4. Model Building and Training:**\n",
        "A Logistic Regression model was instantiated from the scikit-learn library. This model was then trained on the training data (the TF-IDF features and corresponding spam labels). The training process involves the model learning the relationship between the email features and the likelihood of an email being spam.\n",
        "\n",
        "**5. Model Evaluation:**\n",
        "After training, the model's performance was evaluated on the unseen testing data. The following metrics were used:\n",
        "\n",
        "*   **Accuracy:** The proportion of correctly classified emails (both spam and not spam).\n",
        "*   **Precision:** Out of all emails predicted as spam, the proportion that were actually spam. High precision indicates a low rate of false positives (non-spam emails incorrectly classified as spam).\n",
        "*   **Recall:** Out of all actual spam emails, the proportion that were correctly identified by the model. High recall indicates a low rate of false negatives (spam emails incorrectly classified as non-spam).\n",
        "*   **F1-score:** The harmonic mean of precision and recall, providing a balanced measure of the model's performance.\n",
        "\n",
        "The evaluation results on the test set were:\n",
        "*   Accuracy: {accuracy:.4f}\n",
        "*   Precision: {precision:.4f}\n",
        "*   Recall: {recall:.4f}\n",
        "*   F1-score: {f1:.4f}\n",
        "\n",
        "These metrics indicate that the model performed very well on the test set, with high accuracy and strong performance across precision, recall, and F1-score.\n",
        "\n",
        "**6. Making Predictions on New Data:**\n",
        "Finally, the tutorial demonstrated how to use the trained model to predict whether new, unseen emails are spam or not spam. This involves taking the new email text, transforming it using the *same* fitted TF-IDF vectorizer used during training, and then feeding the resulting numerical features into the trained Logistic Regression model to get a spam/not spam prediction.\n",
        "\"\"\"\n",
        "\n",
        "print(summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c282f2d"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The dataset contains 5728 emails with their content and spam classification.\n",
        "*   There were no missing values in the dataset.\n",
        "*   The text data was successfully transformed into a TF-IDF matrix with 5000 features.\n",
        "*   The data was split into an 80% training set (4582 samples) and a 20% testing set (1146 samples).\n",
        "*   A Logistic Regression model was trained on the TF-IDF features and spam labels.\n",
        "*   The trained model achieved high performance on the test set:\n",
        "    *   Accuracy: 0.9808\n",
        "    *   Precision: 0.9963\n",
        "    *   Recall: 0.9276\n",
        "    *   F1-score: 0.9607\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The high precision score suggests the model is very good at correctly identifying emails as spam when it predicts they are spam, which is important for minimizing false positives.\n",
        "*   While the recall is also high, exploring techniques to potentially increase it further without significantly sacrificing precision could be beneficial for capturing more actual spam emails.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The code below demonstrates how to make predictions on new email texts using the trained model.\n",
        "# It first creates a list of new emails, transforms them into TF-IDF features using the same vectorizer\n",
        "# used during training, and then uses the trained Logistic Regression model to predict\n",
        "# whether each email is spam or not.\n",
        "\n",
        "# 1. Create new email texts\n",
        "new_emails = [\n",
        "    \"Subject: special offer just for you! limited time!\", # Likely spam\n",
        "    \"Subject: meeting scheduled for tomorrow at 10am\", # Likely not spam\n",
        "    \"Subject: your account has been compromised - urgent action required\", # Likely spam\n",
        "    \"Subject: project update and next steps\", # Likely not spam\n",
        "    \"Subject: congratulations! you've won a prize!\", # Likely spam\n",
        "]\n",
        "\n",
        "# 2. Transform new email texts using the fitted tfidf_vectorizer\n",
        "new_emails_tfidf = tfidf_vectorizer.transform(new_emails)\n",
        "\n",
        "# 3. Use the trained model to make predictions\n",
        "predictions = model.predict(new_emails_tfidf)\n",
        "\n",
        "# 4. Print the original texts and their predictions\n",
        "print(\"New Email Predictions:\")\n",
        "for email, prediction in zip(new_emails, predictions):\n",
        "    print(f\"Email: {email}\")\n",
        "    print(f\"Prediction: {'Spam' if prediction == 1 else 'Not Spam'}\")\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "id": "oN-tlGDJVFlo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}