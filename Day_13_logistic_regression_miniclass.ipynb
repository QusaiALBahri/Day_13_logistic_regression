{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBeSeEBdcLxlnM46fi0pS0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QusaiALBahri/Day_13_logistic_regression/blob/main/Day_13_logistic_regression_miniclass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a834f43b"
      },
      "source": [
        "## Logistic Regression Tutorial\n",
        "\n",
        "Logistic Regression is a statistical model used for binary classification. It predicts the probability that a given data point belongs to a particular class (e.g., spam or not spam, tumor benign or malignant). Unlike linear regression, which outputs a continuous value, logistic regression uses a sigmoid function to map any real-valued number to a value between 0 and 1, which can be interpreted as a probability.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "*   **Binary Classification:** Classifying data into one of two categories.\n",
        "*   **Sigmoid Function:** A mathematical function that squashes any input value into the range [0, 1]. The formula is:\n",
        "    $$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
        "    where $z$ is the output of a linear combination of features and weights.\n",
        "*   **Decision Boundary:** A threshold (usually 0.5) applied to the sigmoid output to make a final classification. If the probability is above the threshold, the data point is classified into one class; otherwise, it's classified into the other.\n",
        "*   **Cost Function:** Measures the performance of the logistic regression model. A common cost function is the cross-entropy loss.\n",
        "*   **Gradient Descent:** An optimization algorithm used to find the optimal weights and biases that minimize the cost function.\n",
        "\n",
        "Let's walk through a simple example using Python and scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "jLXbabFdxWdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some synthetic data for demonstration\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = (X[:, 0] > 5).astype(int) + np.random.randint(0, 2, 100)  # Create some overlap"
      ],
      "metadata": {
        "id": "PIgjE-VAxa5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.clip(y, 0, 1) # Ensure labels are 0 or 1\n",
        "#print(X,y)"
      ],
      "metadata": {
        "id": "qVAfIYSpxnmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "#print(X_train)\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "-dWScs2Dx60b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "#print(y_pred)\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "6Ou6RqCUzYVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ffc92f2"
      },
      "source": [
        "# Visualize the decision boundary (for a 1D example)\n",
        "x_values = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "y_prob = model.predict_proba(x_values)[:, 1]\n",
        "\n",
        "plt.scatter(X_test, y_test, color='black', zorder=3, edgecolors='white')\n",
        "plt.plot(x_values, y_prob, color='red', linewidth=2)\n",
        "plt.axhline(0.5, color='blue', linestyle='--', label='Decision Boundary')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Probability / Class')\n",
        "plt.title('Logistic Regression Decision Boundary')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88413117"
      },
      "source": [
        "**Explanation of the Code:**\n",
        "\n",
        "1.  **Import Libraries:** We import `numpy` for numerical operations, `matplotlib.pyplot` for plotting, and modules from `sklearn` for splitting data, creating the logistic regression model, and evaluating it.\n",
        "2.  **Generate Data:** We create simple synthetic data where the target variable `y` is roughly determined by whether the feature `X` is greater than 5. We add some noise to make it a bit more realistic.\n",
        "3.  **Split Data:** We split the data into training and testing sets to evaluate how well our model generalizes to unseen data.\n",
        "4.  **Create and Train Model:** We initialize a `LogisticRegression` model and train it using the `fit()` method on the training data.\n",
        "5.  **Make Predictions:** We use the trained model to predict the class labels for the test set using the `predict()` method.\n",
        "6.  **Evaluate Model:** We calculate the accuracy of the model by comparing the predicted labels (`y_pred`) to the actual labels (`y_test`).\n",
        "7.  **Visualize Decision Boundary:** We generate a range of feature values (`x_values`) and use `predict_proba()` to get the predicted probabilities for each value. We then plot these probabilities and the decision boundary at 0.5 to visualize how the model makes classifications based on the feature value.\n",
        "\n",
        "This tutorial provides a basic understanding of logistic regression and how to implement it using scikit-learn. You can extend this to more complex datasets and explore other evaluation metrics and techniques like regularization."
      ]
    }
  ]
}